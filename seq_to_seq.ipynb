{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 million word vectors trained on Common Crawl (600B tokens)\n",
    "embedding_file = open('processed_data/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line of the file contains the number of words in the vocabulary and the size of the vectors. \n",
    "# Each line contains a word followed by its vectors, like in the default fastText text format. \n",
    "# Each value is space separated.\n",
    "# Words are ordered by descending frequency.\n",
    "meta = embedding_file.readline()\n",
    "num_words = int(meta.split()[0])\n",
    "num_dims = int(meta.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1999999\n",
      "Number of dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "# Some 'words' are not picked up by split(). Ignoring them.\n",
    "# dictionary -> word : index\n",
    "# reverse_dictionary -> index : word\n",
    "dictionary = dict()\n",
    "# To store embeddings for each word\n",
    "embeddings = np.zeros((num_words, num_dims))\n",
    "i = 0\n",
    "\n",
    "# start of sequence token\n",
    "dictionary['PAD_token'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# start of sequence token\n",
    "dictionary['Start_of_Sequence'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# end of sequence token\n",
    "dictionary['End_of_Sequence'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# unkown word token\n",
    "dictionary['UNK_token'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# Skip words not separated by split()\n",
    "for line in embedding_file:\n",
    "    items = line.split()\n",
    "    if len(items) == (num_dims + 1):\n",
    "        dictionary[items[0]] = len(dictionary)\n",
    "        embeddings[i] = np.asarray([float(value) for value in items[1:]], dtype=np.float64)\n",
    "        i += 1\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "max_vocab = len(dictionary)\n",
    "embeddings = embeddings[:max_vocab]\n",
    "embedding_file.close()\n",
    "\n",
    "print(\"Total words: %d\" % max_vocab)\n",
    "print(\"Number of dimensions: %d\" % num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary size: 40000\n",
      "Output vocabulary size: 40000\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = 40000\n",
    "output_vocab_size = 40000\n",
    "# Input words and embeddings\n",
    "input_dictionary = dict()\n",
    "input_reverse_dictionary = dict()\n",
    "input_embeddings = np.zeros((input_vocab_size, num_dims))\n",
    "# Output words and embeddings\n",
    "output_dictionary = dict()\n",
    "output_reverse_dictionary = dict()\n",
    "output_embeddings = np.zeros((output_vocab_size, num_dims))\n",
    "# Extract input vocabulary\n",
    "for i in range(input_vocab_size):\n",
    "    word = reverse_dictionary[i]\n",
    "    input_dictionary[word] = i\n",
    "    input_reverse_dictionary[i] = word\n",
    "    input_embeddings[i] = embeddings[i]\n",
    "# Extract output vocabulary\n",
    "for i in range(output_vocab_size):\n",
    "    word = reverse_dictionary[i]\n",
    "    output_dictionary[word] = i\n",
    "    output_reverse_dictionary[i] = word\n",
    "    output_embeddings = embeddings[i]\n",
    "    \n",
    "del dictionary, reverse_dictionary, embeddings\n",
    "print('Input vocabulary size: %d' % input_vocab_size)\n",
    "print('Output vocabulary size: %d' % output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed titles and articles\n",
    "titles = pickle.load(open('processed_data/titles', 'rb'))\n",
    "articles = pickle.load(open('processed_data/articles', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 141549\n",
      "Maximum article size: 80\n",
      "Maximum title size: 37\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens to extract from the beginning of each article\n",
    "max_article_size = 80\n",
    "min_article_size = 30\n",
    "\n",
    "# Convert each title and article to lists of words\n",
    "# Discard if article size is less than minimum or is less than title size\n",
    "# or if title doesn't exist\n",
    "temp1 = list()\n",
    "temp2 = list()\n",
    "max_title_size = 0\n",
    "for i in range(len(articles)):\n",
    "    title = titles[i].split()\n",
    "    article = articles[i].split()[:max_article_size]\n",
    "    if len(title) > max_title_size: max_title_size = len(title)\n",
    "    if (len(article)>=min_article_size) and (len(article)>len(title)) and len(title) > 0:\n",
    "        temp1.append(title)\n",
    "        temp2.append(article)\n",
    "titles = temp1\n",
    "articles = temp2\n",
    "del temp1, temp2\n",
    "\n",
    "print('Total number of samples: %d' % len(articles))\n",
    "print('Maximum article size: %d' % max_article_size)\n",
    "print('Maximum title size: %d' % max_title_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 140549\n",
      "Valid set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Validation set size\n",
    "valid_size = 1000\n",
    "\n",
    "# Separate into training and validation sets\n",
    "valid_titles = titles[:valid_size]\n",
    "valid_articles = articles[:valid_size]\n",
    "train_titles = titles[valid_size:]\n",
    "train_articles = articles[valid_size:]\n",
    "train_size = len(train_articles)\n",
    "del titles, articles\n",
    "\n",
    "print('Training set size: %d' % train_size)\n",
    "print('Valid set size: %d' % valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_max_len = max_article_size\n",
    "output_max_len = max_title_size\n",
    "\n",
    "class BatchGenerator(object): # Needs refactoring\n",
    "    \n",
    "    def __init__(self, article_list, title_list, batch_size):\n",
    "        self._article_list = article_list\n",
    "        self._title_list = title_list\n",
    "        self._list_size = len(article_list) # or title_list\n",
    "        self._batch_size = batch_size\n",
    "        self._input_max_len = input_max_len\n",
    "        self._output_max_len = output_max_len\n",
    "        segment = self._list_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        \n",
    "    def _next_seq(self):\n",
    "        # List to hold articles\n",
    "        seq_list = list()\n",
    "        # List to hold titles\n",
    "        label_list = list()\n",
    "        # Tensor to hold length of each sequence in a batch\n",
    "        seq_lens = torch.cuda.LongTensor(self._batch_size)\n",
    "        # Tensor to hold index of each word of an article\n",
    "        sequence = torch.cuda.LongTensor(self._input_max_len).fill_(0)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            # Each article as a list of words\n",
    "            _list = self._article_list[self._cursor[b]]\n",
    "            length = len(_list)\n",
    "            seq_lens[b] = length\n",
    "            # Place word indices from the left\n",
    "            sequence[:length] = torch.from_numpy(np.asarray([input_dictionary[word] \n",
    "                                 if word in input_dictionary \n",
    "                                 else input_dictionary['UNK_token'] for word in _list]))\n",
    "            seq_list.append(sequence)\n",
    "            # Corresponding title\n",
    "            label_list.append(self._title_list[self._cursor[b]])\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._list_size\n",
    "        if self._batch_size > 1:\n",
    "            # Sort sequences in descending order\n",
    "            seq_lens, indices = torch.sort(seq_lens, descending=True)\n",
    "            seq_list = [seq_list[i] for i in indices]\n",
    "            label_list = [label_list[i] for i in indices]\n",
    "        return seq_list, seq_lens, label_list\n",
    "    \n",
    "    def _labels(self, label_list, hiddenT):\n",
    "        hidden = hiddenT[0]\n",
    "        cell = hiddenT[1]\n",
    "        # List to hold titles\n",
    "        seq_list = list()\n",
    "        # Tensor to hold length of each sequence in a batch\n",
    "        seq_lens = torch.cuda.LongTensor(self._batch_size)\n",
    "        # Tensor to hold index of each word of an article\n",
    "        sequence = torch.cuda.LongTensor(self._output_max_len + 2).fill_(0)\n",
    "        sequence[0] = output_dictionary['Start_of_Sequence']\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            length = len(label_list[b])\n",
    "            seq_lens[b] = length + 2\n",
    "            # Place word indices from the left\n",
    "            sequence[1:length+1] = torch.from_numpy(np.asarray([output_dictionary[word] \n",
    "                                 if word in output_dictionary \n",
    "                                 else output_dictionary['UNK_token'] for word in label_list[b]]))\n",
    "            sequence[length+1] = output_dictionary['End_of_Sequence']\n",
    "            seq_list.append(sequence)\n",
    "        if self._batch_size > 1:\n",
    "            # Sort sequences in descending order\n",
    "            seq_lens, indices = torch.sort(seq_lens, descending=True)\n",
    "            seq_list = [seq_list[i] for i in indices]\n",
    "            # Sort last hidden layer of encoder\n",
    "            hidden = hidden[:,indices]\n",
    "            cell = cell[:,indices]\n",
    "        return seq_list, seq_lens, (hidden, cell)\n",
    "            \n",
    "\n",
    "train_batches = BatchGenerator(train_articles, train_titles, batch_size)\n",
    "valid_batches = BatchGenerator(valid_articles, valid_titles, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    train_batches._next_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embed): Embedding(40000, 300)\n",
       "  (lstm): LSTM(300, 512, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of hidden units in each hidden layer\n",
    "encoder_hidden_size = 512\n",
    "# Number of recurrent layers in encoder\n",
    "encoder_num_layers = 2\n",
    "# Encoder dropout\n",
    "encoder_dropout = 0\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, vocab_size, hidden_size, num_layers, dropout):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        \n",
    "        # Hidden layer and cell state of model\n",
    "        # Initialize before calling model\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(self._vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, self._hidden_size, \n",
    "                            self._num_layers, dropout=self._dropout)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.cuda.FloatTensor(self._num_layers, batch_size,\n",
    "                    self._hidden_size).fill_(0), requires_grad=False), \n",
    "                Variable(torch.cuda.FloatTensor(self._num_layers, batch_size, \n",
    "                    self._hidden_size).fill_(0), \n",
    "                         requires_grad=False))\n",
    "    \n",
    "    def forward(self, seq_list, seq_lens):\n",
    "        batch_size = len(seq_list)\n",
    "        inputs = Variable(torch.cuda.FloatTensor(torch.max(seq_lens), batch_size, \n",
    "                        num_dims), requires_grad=False)\n",
    "        for b in range(batch_size):\n",
    "            inputs[:,b] = self.embed(Variable(seq_list[b], requires_grad=False).cuda())\n",
    "        packed_input = pack_padded_sequence(inputs, seq_lens.cpu().numpy())\n",
    "        output, self.hidden = self.lstm(packed_input, (self.hidden))\n",
    "        # Final hidden state\n",
    "        return self.hidden\n",
    "    \n",
    "encoder = EncoderRNN(input_embeddings, input_vocab_size,\n",
    "                     encoder_hidden_size, encoder_num_layers, encoder_dropout)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(40000, 300)\n",
       "  (lstm): LSTM(300, 512, num_layers=2)\n",
       "  (hidden2word): Linear(in_features=512, out_features=40000)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of hidden units in each hidden layer\n",
    "decoder_hidden_size = 512\n",
    "# Number of recurrent layers in encoder\n",
    "decoder_num_layers = 2\n",
    "# Encoder dropout\n",
    "decoder_dropout = 0\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, vocab_size, hidden_size, num_layers, dropout):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        \n",
    "        # Hidden layer and cell state of model\n",
    "        # Initialize before calling model\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(self._vocab_size, num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, self._hidden_size, \n",
    "                            self._num_layers, dropout=self._dropout)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.hidden2word = nn.Linear(self._hidden_size, self._vocab_size)\n",
    "        \n",
    "    def forward(self, seq_list, seq_lens):\n",
    "        batch_size = len(seq_list)\n",
    "        inputs = Variable(torch.cuda.FloatTensor(torch.max(seq_lens-1), batch_size, \n",
    "                    num_dims), requires_grad=False)\n",
    "        for b in range(batch_size):\n",
    "            input = seq_list[b][:-1]\n",
    "            inputs[:,b] = self.embed(Variable(input, requires_grad=False).cuda())\n",
    "        packed_input = pack_padded_sequence(inputs, seq_lens.cpu().numpy())\n",
    "        packed_output, self.hidden = self.lstm(packed_input, (self.hidden))\n",
    "        output, _ = pad_packed_sequence(packed_output)\n",
    "        output = self.hidden2word(output)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "        \n",
    "decoder = DecoderRNN(output_embeddings, output_vocab_size,\n",
    "                     decoder_hidden_size, decoder_num_layers, decoder_dropout)\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    # Clear model gradients\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Clear hidden state of LSTM\n",
    "    encoder.hidden = encoder.init_hidden(batch_size)\n",
    "    \n",
    "    # Get batch_size number of sequences, lengths of sequences and labels (titles)\n",
    "    # seq_list, seq_lens sorted. labels not sorted\n",
    "    seq_list, seq_lens, label_list = train_batches._next_seq()\n",
    "    hiddenT = encoder(seq_list, seq_lens)\n",
    "    # Sort labels and last hidden layer of encoder accordingly    \n",
    "    label_list, label_lens, hiddenT = train_batches._labels(label_list, hiddenT)\n",
    "    decoder.hidden = hiddenT\n",
    "    output = decoder(label_list, label_lens)\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Variable containing:\n",
      "(  0  ,.,.) = \n",
      "  1.7053e-02  2.4089e-02 -9.1320e-04  ...   4.5620e-03 -1.6555e-02  2.2388e-02\n",
      "  1.7053e-02  2.4089e-02 -9.1320e-04  ...   4.5620e-03 -1.6555e-02  2.2388e-02\n",
      "  1.7053e-02  2.4089e-02 -9.1320e-04  ...   4.5620e-03 -1.6555e-02  2.2388e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.7053e-02  2.4089e-02 -9.1320e-04  ...   4.5620e-03 -1.6555e-02  2.2388e-02\n",
      "  1.7053e-02  2.4089e-02 -9.1320e-04  ...   4.5620e-03 -1.6555e-02  2.2388e-02\n",
      "  1.2575e-02  2.2359e-02  1.1125e-03  ...   6.2122e-03 -2.1434e-02  2.3169e-02\n",
      "\n",
      "(  1  ,.,.) = \n",
      "  1.6120e-02  1.9810e-02 -2.3937e-04  ...   7.7645e-03 -1.6024e-02  2.8672e-02\n",
      "  1.6120e-02  1.9810e-02 -2.3937e-04  ...   7.7645e-03 -1.6024e-02  2.8672e-02\n",
      "  1.6120e-02  1.9810e-02 -2.3937e-04  ...   7.7645e-03 -1.6024e-02  2.8672e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.6120e-02  1.9810e-02 -2.3937e-04  ...   7.7645e-03 -1.6024e-02  2.8672e-02\n",
      "  1.6120e-02  1.9810e-02 -2.3937e-04  ...   7.7645e-03 -1.6024e-02  2.8672e-02\n",
      "  1.2428e-02  1.9162e-02  1.1118e-03  ...   8.9284e-03 -1.9758e-02  2.9063e-02\n",
      "\n",
      "(  2  ,.,.) = \n",
      "  1.5668e-02  1.7181e-02 -1.4833e-03  ...   9.1119e-03 -1.4845e-02  3.1721e-02\n",
      "  1.5668e-02  1.7181e-02 -1.4833e-03  ...   9.1119e-03 -1.4845e-02  3.1721e-02\n",
      "  1.5668e-02  1.7181e-02 -1.4833e-03  ...   9.1119e-03 -1.4845e-02  3.1721e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.5668e-02  1.7181e-02 -1.4833e-03  ...   9.1119e-03 -1.4845e-02  3.1721e-02\n",
      "  1.5668e-02  1.7181e-02 -1.4833e-03  ...   9.1119e-03 -1.4845e-02  3.1721e-02\n",
      "  1.3041e-02  1.6977e-02 -6.4907e-04  ...   9.8279e-03 -1.7545e-02  3.1943e-02\n",
      " ...  \n",
      "\n",
      "( 19  ,.,.) = \n",
      "  1.7370e-02  1.4224e-02 -8.2049e-03  ...   7.2719e-03 -1.0412e-02  3.3043e-02\n",
      "  1.7370e-02  1.4224e-02 -8.2049e-03  ...   7.2719e-03 -1.0412e-02  3.3043e-02\n",
      "  1.7370e-02  1.4224e-02 -8.2049e-03  ...   7.2719e-03 -1.0412e-02  3.3043e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "\n",
      "( 20  ,.,.) = \n",
      "  1.7371e-02  1.4222e-02 -8.2039e-03  ...   7.2698e-03 -1.0411e-02  3.3042e-02\n",
      "  1.7371e-02  1.4222e-02 -8.2039e-03  ...   7.2698e-03 -1.0411e-02  3.3042e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "\n",
      "( 21  ,.,.) = \n",
      "  1.7372e-02  1.4221e-02 -8.2032e-03  ...   7.2686e-03 -1.0410e-02  3.3041e-02\n",
      "  1.7372e-02  1.4221e-02 -8.2032e-03  ...   7.2686e-03 -1.0410e-02  3.3041e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "  9.2617e-03  2.5083e-02  1.0780e-03  ...  -1.0913e-02 -1.4255e-02  1.7247e-02\n",
      "[torch.cuda.FloatTensor of size 22x128x40000 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
