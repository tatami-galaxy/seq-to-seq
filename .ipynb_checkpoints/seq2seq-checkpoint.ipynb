{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 million word vectors trained on Common Crawl (600B tokens)\n",
    "embedding_file = open('processed_data/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line of the file contains the number of words in the vocabulary and the size of the vectors. \n",
    "# Each line contains a word followed by its vectors, like in the default fastText text format. \n",
    "# Each value is space separated.\n",
    "# Words are ordered by descending frequency.\n",
    "meta = embedding_file.readline()\n",
    "num_words = int(meta.split()[0])\n",
    "num_dims = int(meta.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some 'words' are not picked up by split(). Ignoring them.\n",
    "# dictionary -> word : index\n",
    "# reverse_dictionary -> index : word\n",
    "dictionary = dict()\n",
    "# To store embeddings for each word\n",
    "embeddings = np.zeros((num_words, num_dims))\n",
    "i = 0\n",
    "\n",
    "# start of sequence token\n",
    "dictionary['<pad>'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# start of sequence token\n",
    "dictionary['<sos>'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# end of sequence token\n",
    "dictionary['<eos>'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# unkown word token\n",
    "dictionary['<unk>'] = len(dictionary)\n",
    "embeddings[i] = np.random.rand(num_dims)\n",
    "i += 1\n",
    "\n",
    "# Skip words not separated by split()\n",
    "for line in embedding_file:\n",
    "    items = line.split()\n",
    "    if len(items) == (num_dims + 1):\n",
    "        dictionary[items[0]] = len(dictionary)\n",
    "        embeddings[i] = np.asarray([float(value) for value in items[1:]], dtype=np.float64)\n",
    "        i += 1\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "max_vocab = len(dictionary)\n",
    "embeddings = embeddings[:max_vocab]\n",
    "embedding_file.close()\n",
    "\n",
    "print(\"Total words: %d\" % max_vocab)\n",
    "print(\"Number of dimensions: %d\" % num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 40000\n",
    "output_vocab_size = 40000\n",
    "# Input words and embeddings\n",
    "input_dictionary = dict()\n",
    "input_reverse_dictionary = dict()\n",
    "input_embeddings = np.zeros((input_vocab_size, num_dims))\n",
    "# Output words and embeddings\n",
    "output_dictionary = dict()\n",
    "output_reverse_dictionary = dict()\n",
    "output_embeddings = np.zeros((output_vocab_size, num_dims))\n",
    "# Extract input vocabulary\n",
    "for i in range(input_vocab_size):\n",
    "    word = reverse_dictionary[i]\n",
    "    input_dictionary[word] = i\n",
    "    input_reverse_dictionary[i] = word\n",
    "    input_embeddings[i] = embeddings[i]\n",
    "# Extract output vocabulary\n",
    "for i in range(output_vocab_size):\n",
    "    word = reverse_dictionary[i]\n",
    "    output_dictionary[word] = i\n",
    "    output_reverse_dictionary[i] = word\n",
    "    output_embeddings = embeddings[i]\n",
    "    \n",
    "del dictionary, reverse_dictionary, embeddings\n",
    "print('Input vocabulary size: %d' % input_vocab_size)\n",
    "print('Output vocabulary size: %d' % output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed titles and articles\n",
    "titles = pickle.load(open('processed_data/titles', 'rb'))\n",
    "articles = pickle.load(open('processed_data/articles', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
