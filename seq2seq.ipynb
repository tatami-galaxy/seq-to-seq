{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import revtok\n",
    "from torchtext.data import BucketIterator\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields for articles(text) and titles(summary)\n",
    "text = data.ReversibleField(sequential=True, tokenize=None, include_lengths=True)\n",
    "summary = data.ReversibleField(sequential=True, tokenize=None, \n",
    "                               include_lengths=True, eos_token='<eos>')\n",
    "\n",
    "# Creating training and validation datasets\n",
    "train, valid = data.TabularDataset.splits(\n",
    "    path='./processed_data/', train='train_dataset.csv',\n",
    "    validation='valid_dataset.csv', skip_header=True, \n",
    "    format='csv', fields=[('article', text), ('title', summary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedidngs to build vocabulary\n",
    "#input_vectors = vocab.Vectors('crawl-300d-2M-input.vec')\n",
    "#output_vectors = vocab.Vectors('crawl-300d-2M-output.vec')\n",
    "\n",
    "#text.build_vocab(train, vectors=input_vectors)\n",
    "text.build_vocab(train, vectors='glove.6B.300d')\n",
    "\n",
    "#summary.build_vocab(train, vectors=input_vectors)\n",
    "summary.build_vocab(train, vectors='glove.6B.300d')\n",
    "\n",
    "# Number of dimensions of embeddings\n",
    "num_dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Iterator for generating batches\n",
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (train, valid), \n",
    "    (batch_size, 1),\n",
    "    device=0,\n",
    "    #sort=True, # sort in ascending order of lenghts\n",
    "    sort_key=lambda x: len(x.article),\n",
    "    sort_within_batch=True, # sort each batch in descending order of article length\n",
    "    repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing embedding of all zeros for UNK with a random embedding\n",
    "_ = torch.nn.init.normal(text.vocab.vectors[0], mean=0, std=0.05) \n",
    "_ = torch.nn.init.normal(summary.vocab.vectors[0], mean=0, std=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embed): Embedding(114618, 300)\n",
       "  (lstm): LSTM(300, 512, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input vocabulary\n",
    "input_vocab = text.vocab\n",
    "# Number of hidden units in each hidden layer\n",
    "encoder_hidden_size = 512\n",
    "# Number of recurrent layers in encoder\n",
    "encoder_num_layers = 2\n",
    "# Encoder dropout\n",
    "encoder_dropout = 0\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, hidden_size, num_layers, dropout):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self._vocab = vocab\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        \n",
    "        # Hidden layer and cell state of model\n",
    "        # Initialize before calling model\n",
    "        self.hidden = None\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(len(self._vocab), num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(self._vocab.vectors)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm module\n",
    "        self.lstm = nn.LSTM(num_dims, self._hidden_size, \n",
    "                            self._num_layers, dropout=self._dropout)\n",
    "        self.lstm.cuda()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.cuda.FloatTensor(self._num_layers, batch_size,\n",
    "                    self._hidden_size).fill_(0), requires_grad=False), \n",
    "                Variable(torch.cuda.FloatTensor(self._num_layers, batch_size, \n",
    "                    self._hidden_size).fill_(0), \n",
    "                         requires_grad=False))\n",
    "    \n",
    "    def forward(self, batch_data, sequence_lengths):\n",
    "        batch_size = len(sequence_lengths)\n",
    "        # Embedding lookup\n",
    "        input = self.embed(batch_data)\n",
    "        # packed_input is of size Txbx*\n",
    "        # where T is the length of longest sequence\n",
    "        # b is batch size\n",
    "        # batch is sorted in descending order of sequence lengths\n",
    "        packed_input = pack_padded_sequence(input, list(sequence_lengths))\n",
    "        packed_output, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "        # Final hidden state\n",
    "        return self.hidden\n",
    "    \n",
    "encoder = EncoderRNN(input_vocab, encoder_hidden_size, \n",
    "                     encoder_num_layers, encoder_dropout)\n",
    "encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(43919, 300)\n",
       "  (lstmCell1): LSTMCell(300, 512)\n",
       "  (lstmCell2): LSTMCell(512, 512)\n",
       "  (linear_transform): Linear(in_features=512, out_features=43919, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output vocabulary\n",
    "output_vocab = summary.vocab\n",
    "# Number of hidden units in each hidden layer\n",
    "decoder_hidden_size = 512\n",
    "# Number of recurrent layers in encoder\n",
    "decoder_num_layers = 2\n",
    "# Encoder dropout\n",
    "decoder_dropout = 0\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, hidden_size, num_layers, dropout):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self._vocab = vocab\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        \n",
    "        # Lookup table that stores word embeddings\n",
    "        self.embed = nn.Embedding(len(self._vocab), num_dims).cuda()\n",
    "        self.embed.weight.data.copy_(self._vocab.vectors)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        # Pytorch lstm cell for first layer\n",
    "        self.lstmCell1 = nn.LSTMCell(num_dims, self._hidden_size)\n",
    "        self.lstmCell1.cuda()\n",
    "        \n",
    "        # Pytorch lstm cell for second layer\n",
    "        self.lstmCell2 = nn.LSTMCell(self._hidden_size, self._hidden_size)\n",
    "        self.lstmCell2.cuda()\n",
    "        \n",
    "        # Linear transformation \n",
    "        self.linear_transform = nn.Linear(self._hidden_size, len(self._vocab))\n",
    "    \n",
    "    def forward(self, input, hidden1, hidden2):\n",
    "        input = self.embed(input) \n",
    "        hidden1 = self.lstmCell1(input, hidden1) \n",
    "        hidden2 = self.lstmCell2(hidden1[0], hidden2)\n",
    "        output = self.linear_transform(hidden2[0])\n",
    "        return hidden1, hidden2, output\n",
    "    \n",
    "decoder = DecoderRNN(output_vocab, decoder_hidden_size, \n",
    "                     decoder_num_layers, decoder_dropout)\n",
    "decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distribution(distribution):\n",
    "# Sample one element from a distribution assumed to be an array of normalized probabilities\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def predict_by_sampling(output, batch_size):\n",
    "    output = output.cpu() \n",
    "    next_input = torch.cuda.LongTensor(batch_size)\n",
    "    # output has shape (batch_size, vocab_size)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    output = softmax(output)\n",
    "    for b in range(batch_size): \n",
    "        next_input[b] = sample_distribution(output[b].data)\n",
    "    return Variable(next_input)\n",
    "\n",
    "def most_likely(output, batch_size):\n",
    "    if batch_size > 1:\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1, dim=1)\n",
    "    else: \n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        output = softmax(output)\n",
    "        _, next_input = torch.topk(output, 1)\n",
    "    return next_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam(object):\n",
    "    \n",
    "    def __init__(self, beam_width, batch_size, vocab):\n",
    "        self._beam_width = beam_width\n",
    "        self._batch_size = batch_size\n",
    "        self._vocab = vocab\n",
    "        self.outputs = torch.FloatTensor(self._beam_width, self._batch_size, len(self._vocab))\n",
    "        \n",
    "    def beam_search(self, ):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "beam_size = 3\n",
    "\n",
    "# Filter parameters that do not require gradients\n",
    "encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "# Optimizers\n",
    "encoder_optimizer = torch.optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "# Loss function\n",
    "# By default the losses are averaged over observations for each minibatch\n",
    "# When reduce is False returns a loss per batch element instead  \n",
    "#loss_function = nn.CrossEntropyLoss(reduce=False)\n",
    "loss_function = nn.CrossEntropyLoss(size_average=False, ignore_index=1).cuda() # 1 is the index of <pad>\n",
    "\n",
    "def train_model(batch):\n",
    "    loss = 0\n",
    "    # Clear model gradients\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    # Clear optimizer gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Clear hidden state of LSTM\n",
    "    encoder.hidden = encoder.init_hidden(batch_size)\n",
    "    # articles, titles are LongTensor vairables of shape (max_sequence_length, batch_size)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, art_lengths = batch.article\n",
    "    titles, tit_lengths = batch.title\n",
    "    hiddenT = encoder(articles, art_lengths)\n",
    "    # Seperate hidden states corresponding to the the two layers of the encoder\n",
    "    hidden1 = (hiddenT[0][0], hiddenT[1][0])\n",
    "    hidden2 = (hiddenT[0][1], hiddenT[1][1])\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(beam_size):\n",
    "        inputs.append(Variable(torch.cuda.LongTensor(batch_size).fill_(1))) # 1 is the index of <pad> ##error?\n",
    "    # Initial input is a tensor of all <pad> tokens with zero embeddings\n",
    "    input = Variable(torch.cuda.LongTensor(batch_size).fill_(1)) # 1 is the index of <pad> ##error?\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(tit_lengths)):\n",
    "        hidden1, hidden2, output = decoder(input, hidden1, hidden2)\n",
    "        \n",
    "        #input = predict_by_sampling(output, batch_size)\n",
    "        #input = most_likely(output, batch_size)\n",
    "        \n",
    "        \n",
    "        loss += loss_function(output, titles[t])\n",
    "    \n",
    "    loss = loss/torch.sum(tit_lengths)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 3 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 3))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average minibatch loss at step 100: 8.445\n",
      "Average minibatch loss at step 200: 8.056\n",
      "Average minibatch loss at step 300: 8.104\n",
      "Average minibatch loss at step 500: 7.692\n",
      "Average minibatch loss at step 600: 8.008\n",
      "Average minibatch loss at step 700: 7.761\n",
      "Average minibatch loss at step 800: 7.610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1986cf9f752b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average minibatch loss at step %d: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-48f3b349035f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_by_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m#input = most_likely(output, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-102f9c4cf683>\u001b[0m in \u001b[0;36mpredict_by_sampling\u001b[0;34m(output, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m### Slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnext_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-102f9c4cf683>\u001b[0m in \u001b[0;36msample_distribution\u001b[0;34m(distribution)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "step = 1\n",
    "\n",
    "for batch in train_iter:\n",
    "    if torch.max(batch.article[1]) > 80: \n",
    "        step += 1\n",
    "        continue\n",
    "    loss = train_model(batch)\n",
    "    if step % 100 == 0:\n",
    "        valid_loss = validatio\n",
    "        print('Average minibatch loss at step %d: %.3f' % (step, loss))\n",
    "    if step % 1000 == 0 and step > 1000:\n",
    "        adjust_learning_rate(encoder_optimizer, step // 1000)\n",
    "        adjust_learning_rate(decoder_optimizer, step // 1000)\n",
    "    step += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is   over   iowa   if   fed   leaves   new   giving   merkels   to   trump   a   on   clashes   outlook   texas   deal   new   at   is   groupie   terrapattern   american   university   serious   hardware   one   to   flunky   your   and   young  eos\n",
      " two   allowed   board   your   the   eleven   private   democratic   but   horsemen   jarmuschs   dicey  eos\n",
      " trump   summit   america   suspected   conway   we   vx   pounds   occupies   law   deal   fundamentals  eos\n",
      " rightwing   company   cuomo   in   to   rhetoric   food   of   be   lock   stone   did   is   immigration   obama   remarks   could   girlfriends   storch   woman   unsheltered   nypost   up   home   burundi   street   all  eos\n",
      " vetoes   using   bush   worlds   after   administration   weiszs   us   obama   chums   sea   fake   is   of   to   release  eos\n",
      " this   wasnt   if   image   freezer   for  eos\n",
      " report   gop   zika   be   new   hundred   mcphee   for   in   wrecks   is   government   online   opec   be   been   with   right   us   xylitol  eos\n",
      " backlash   hell   treats   at   cash   unilateral   percent   to   to   cronies   lets   emo   supervised   the   government   trump   the   tech   news   the   cheerleader   labor   legal   likely   might   bottlecap   shows   uncensored  >   year   used  eos\n",
      " giants   race   five   bill   facebook   clue   you   general   florida   spiders   i   land   choice   search   borrowers   to   his   in   need   embrace   to   campaign   stocks   for   house   belgium   forth   election   no   you  eos\n",
      " republican   of   tech   as   desert   three   cocoa   law   that   by   billion   ads   in   unfixable   final   in   to   dynastic   airfares   clinton   day   disenfranchises   rahims   deafens   because   at   not   attacks   dear   decapitates   allowing   huffed   wynter   usa   just   putting   ahead   on   slack   twenty   make   out   final   in   game   to   pleaser   a   first   league   nasa   white   nra   with   seventeen   a   may   staffers  eos\n",
      " woman   role   no   texas   do   support   destroy   celebrate   lousiness   party   paranormal   girl   to   will   in   law   been   nyfw   telling   dies   a   new   with   the   emmett   opposing   protesters   plan   it   irvings   longer   the   daily   a   realizes   at   eight   believed   scalias   sue   ohio   dont   the   crayola   everyone   news   tinge   people   furs   gains   we   two   without   to   dirtiest   james   protestor   those   haunting   its   cant   a   to   releases   the   its   rebuttals   senator   you   many   agreement   callan   as   before   its   growth   hammering   black   style   manus   dirtbag   people   of   pull   turbulence   of   nine   slips   goes   assaulted   on   policy   father   bill   in   for   aim   specialists   trump   policy  eos\n",
      " atlantic   rep   splash   taken   paris   trumps   patriots   authorities   worshipers   lgbtrump   we   announces   government   be   to   than   same   donald  eos\n",
      " restores   muhammad   nightmare   in   nascar   most   performs   two   sanders   are   selfie   christie   game   role   report  eos\n",
      " minister   oscars   trump   fifteen   the   jumpers   middle   vegetable   planned   –   realitys   prison   woman   to   one   widow   over  eos\n",
      " woman   sanders   cancer   chuck   baltics   things   graydon   four   for   james   receiver   peppers   businesswise   fix   plan   art   places   blocked   me   satirist  eos\n",
      " ben   of   off   middle   mexico   grin   at   hens   their  eos\n",
      " fact   trump   of   does   government   milo   that   times   syria   china  eos\n",
      " korean   shams   to   political   kills   repeal   tough   olympian   unbelievably   media   flying   history   liverpool   for   forty   trump   shazam   yahoo   damage   lepage   chugged   with   wound   before   a   uss   korea  eos\n",
      " marissa   plucky   trump   noticed   burp   an   m   to   in   appears   twenty   havanawood   first   country  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8aae32d50ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#input = predict_by_sampling(output, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_by_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-102f9c4cf683>\u001b[0m in \u001b[0;36mpredict_by_sampling\u001b[0;34m(output, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# output has shape (batch_size, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m### Slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in valid_iter:\n",
    "    encoder.hidden = encoder.init_hidden(1)\n",
    "    # articles, titles are LongTensor vairables of shape (max_sequence_length, batch_size)\n",
    "    # containig word indices from the respective vocabs\n",
    "    # lengths are LongTensor varibles of shape batch_size containing\n",
    "    # lengths of all the sequences in the batch\n",
    "    articles, art_lengths = batch.article\n",
    "    titles, tit_lengths = batch.title\n",
    "    hiddenT = encoder(articles, art_lengths)\n",
    "    # Seperate hidden states corresponding to the the two layers of the encoder\n",
    "    hidden1 = (hiddenT[0][0], hiddenT[1][0])\n",
    "    hidden2 = (hiddenT[0][1], hiddenT[1][1])\n",
    "    # Initial input is a tensor of all <pad> tokens with zero embeddings\n",
    "    input = Variable(torch.cuda.LongTensor(1).fill_(1)) # 1 is the index of <pad>\n",
    "    # Looping over all the sequences\n",
    "    for t in range(torch.max(tit_lengths)):\n",
    "        hidden1, hidden2, output = decoder(input, hidden1, hidden2)\n",
    "        \n",
    "        #input = predict_by_sampling(output, batch_size)\n",
    "        input = predict_by_sampling(output, 1)\n",
    "        word = summary.vocab.itos[input.data.cpu().numpy()[0]]\n",
    "        if word == '<eos>': \n",
    "            print('eos')\n",
    "            break\n",
    "        print(word+' ', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for name, param in encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
